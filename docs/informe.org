#+SETUPFILE: ./org/theme-readtheorg.setup
#+SETUPFILE: ./org/setup.org

#+BEGIN_EXPORT latex
\begin{titlepage}
    \hfill\includegraphics[width=6cm]{assets/logofiuba.jpg}
    \centering
    \vfill
    \Huge \textbf{Trabajo Práctico 1}
    \vskip0.5cm
    \Large \textbf{Grupo}
    \vskip2cm
    \Large [75.73/TB034] Arquitectura del Software \\
    Segundo cuatrimestre de 2025\\
    \vfill
    \begin{center}
    \begin{tabular}{ | l | l | l | }
      \hline
      Alumno & Padrón & Email \\ \hline
      CASTRO MARTINEZ, Jose Ignacio & 106957 & jcastrom@fi.uba.ar \\ \hline
      DEALBERA, Pablo Andres & 106858 & pdealbera@fi.uba.ar \\ \hline
      FIGUEROA RODRIGUEZ, Andrea & 110450 & afigueroa@fi.uba.ar \\ \hline
      RICALDI REBATA, Brayan Alexander & 103344 & bricaldi@fi.uba.ar \\ \hline
    \end{tabular}
    \end{center}
    \vfill
\end{titlepage}
\renewcommand{\contentsname}{Índice}
\tableofcontents
\newpage
\definecolor{bg}{rgb}{0.95,0.95,0.95}
#+END_EXPORT

* Introducción

** Contexto (startup arVault).

** Objetivos del TP.

** Alcance del análisis.

* Atributos de calidad (QA) identificados


** Disponibilidad

Al ser un servicio de exchange de monedas, asumimos que es un servicio que se utiliza durante todos los dias habiles de la semana en horario cambiario. Por lo tanto, es importante que el servicio se encuentre disponible durante esos horarios para no perder clientes.

Además, dado el contexto en el que queremos recuperar la confianza de los usuarios y remontar la reputación, el sistema debe ser altamente accesible para los usuarios y permitir realizar correctamente sus operaciones respetando tiempos razonables de respuesta.
** Escalabilidad (Elasticidad)

La escalabilidad, y en particular la elasticidad, constituyen un atributo de calidad crítico para el servicio de intercambios de arVault. Esto se debe a que la infraestructura del sistema debe ser capaz de adaptarse dinámicamente a variaciones en la demanda de usuarios.

En el contexto del negocio, es esperable la aparición de picos significativos de demanda en momentos específicos (por ejemplo, en la apertura y cierre del horario cambiario), así como también períodos de baja o nula actividad. A ello se suma que, dado que el servicio busca captar rápidamente un gran volumen de nuevos usuarios, especialmente tras campañas de promoción destinadas a revertir percepciones negativas de experiencias pasadas, existe el riesgo de enfrentar aumentos inesperados de tráfico.

Si el sistema careciera de elasticidad, estos picos de operaciones de cambio de moneda podrían derivar en saturación de recursos, lo que a su vez ocasionaría demoras, rechazos de transacciones o caídas del servicio. Dichos incidentes afectarían de manera directa la reputación de la empresa, un aspecto considerado prioritario en función de los objetivos actuales y de las expectativas de los stakeholders.

** Performance

El atributo de calidad Performance, y en particular el User-Perceived Performance, adquiere relevancia crítica en el servicio de intercambio de monedas de arVault, para sustentar esta afirmación nos basamos en el siguiente análisis del contexto y antecedentes brindados:

Tras el lanzamiento de la funcionalidad, se registraron reclamos de usuarios relacionados con demoras y fallas en la ejecución de operaciones de cambio, lo que ha derivado en reseñas negativas y pérdida de confianza en la plataforma. En un contexto donde la empresa necesita con urgencia atraer nuevas rondas de inversión, estas deficiencias de rendimiento representan un riesgo directo, ya que los potenciales inversores han condicionado su apoyo a la realización de mejoras en la calidad del servicio.

En una aplicación financiera, la percepción de agilidad y confiabilidad en la respuesta del sistema es esencial: tiempos de espera excesivos o transacciones fallidas afectan la experiencia de los usuarios y minan la credibilidad de la plataforma. Aunque el diferencial de arVault reside en ofrecer tasas de cambio más convenientes que la competencia, dicho valor se ve neutralizado si el servicio de intercambio no responde con la rapidez y estabilidad que los clientes esperan.

Por ello, la mejora del User-Perceived Performance se presenta como un paso imprescindible no solo para recuperar la confianza de los usuarios actuales, sino también para restaurar la reputación de la empresa ante el mercado y viabilizar la captación de nuevos inversores, garantizando así la continuidad y evolución del negocio.

** Visibilidad

El valor de este atributo de calidad es más indirecto pero estratégico pues permite entender el comportamiento real del sistema, identificar cuellos de botella de performance, localizar errores en operaciones de cambio y detectar patrones de saturación que anticipen problemas de disponibilidad o escalabilidad. Es decir, la visibilidad no impacta de forma inmediata en la experiencia del usuario, pero habilita a los arquitectos y al equipo técnico a diagnosticar, mejorar y sostener los otros atributos de calidad prioritarios.

* Arquitectura base
** Análisis de la influencia de decisiones de diseño en los QA's
En la presente sección se explorarán las decisiones de diseño identificadas en la arquitectura base y su impacto sobre los atributos de calidad estudiados.

** Incorporación del stack de monitoreo cAdvisor + Artillery + StatsD + Graphite + Grafana

El sistema actual presenta un stack de observabilidad para medir, almacenar y visualizar métricas en tiempo real. En concreto:
- cAdvisor: mide métricas de contenedores (CPU, memoria, etc.)
- Artillery: genera carga (testing de rendimiento)
- StatsD + Graphite: recolectan y almacenan métricas
- Grafana: visualiza las métricas

Esta decisión implicó agregar tres nuevos contenedores, configuraciones adicionales, puertos, conexiones en red internas de Docker y dependencias entre servicios, lo cual impactó de diferentes formas a distintos *stakeholders*.

*** Impactos identificados

- *Carga cognitiva alta:*  
  Implicó que desarrolladores y arquitectos del sistema deban comprender cómo se conectan las herramientas, qué hace cada una y cómo interpretar los datos generados.  
  Esto impacta la *usabilidad interna* (para el desarrollador), la *manejabilidad* y la *simplicidad* del sistema, que originalmente se componía solo de un backend y un proxy inverso (Nginx).

- *Complejidad operativa:*  
  Más contenedores implican más puertos, configuraciones y *logs* extensos (en particular en el entorno local del trabajo práctico), lo cual aumenta el esfuerzo de *debugging* y de gestión general.  
  Esto afecta negativamente la *manejabilidad*, dado que se incrementa la complejidad operativa y el tiempo requerido para mantener el sistema.

- *Evaluación y visibilidad del comportamiento del sistema:*  
  Cuando el stack se encuentra correctamente configurado, las métricas permiten monitorear el rendimiento, detectar cuellos de botella y observar cómo interactúan los distintos componentes.  
  Esto mejora la *visibilidad* del sistema y favorece la *confiabilidad*, ya que permite anticipar fallos o anomalías de comportamiento.  
  No obstante, la dependencia entre múltiples herramientas introduce el riesgo de obtener una visibilidad incompleta si alguno de los servicios del stack (por ejemplo, Graphite o StatsD) deja de funcionar.

- *Afectación a la disponibilidad:*  
  El aumento en la cantidad de servicios dependientes implica más puntos de falla. Si Graphite o StatsD se detienen, Grafana dejará de mostrar información actualizada.  
  Además, el tiempo de despliegue y recuperación ante fallos se incrementa, afectando la *disponibilidad* de manera negativa, sobre todo en entornos locales.

- *Apoyo a la testeabilidad y diagnóstico:*  
  El stack de monitoreo potencia la capacidad de análisis durante pruebas de rendimiento (por ejemplo, al utilizar Artillery y observar las métricas en Grafana).  
  Esto facilita la identificación de comportamientos anómalos y la validación de la estabilidad del sistema, mejorando la *testeabilidad*.  
  Sin embargo, la infraestructura adicional necesaria para habilitar el monitoreo también introduce complejidad en el entorno de prueba, lo que puede dificultar la reproducibilidad y el control de los experimentos.

- *Seguridad y aislamiento:*  
  Añadir más servicios amplía la superficie de ataque, ya que cada contenedor es un proceso escuchando en distintos puertos internos.  
  Esto impacta la *seguridad operativa*, aunque su efecto sea poco relevante en entornos locales de desarrollo.

- *Impacto en la portabilidad del sistema:*  
  La containerización permite desplegar el stack completo en distintos entornos con relativa facilidad, lo cual favorece la *portabilidad técnica*.  
  Sin embargo, la fuerte interdependencia entre servicios y las configuraciones específicas de red, puertos y volúmenes reducen la *portabilidad práctica*, dado que pequeñas diferencias en la infraestructura pueden afectar el funcionamiento o requerir ajustes manuales.

- *Interoperabilidad y acoplamiento tecnológico:*  
  El uso de protocolos y herramientas estandarizadas (UDP, HTTP, Grafana, StatsD) favorece la *interoperabilidad* del sistema, tanto entre sus propios componentes como con futuras herramientas externas de monitoreo.

** Impactos del modelo de persistencia elegido

El modelo de persistencia implementado en el sistema consiste en mantener en memoria el estado de los datos y, periódicamente, volcar dicho estado a archivos JSON almacenados localmente en la carpeta ~~/state/~~.  
Esto implica que la persistencia está acoplada directamente a la instancia del servidor —es decir, se trata de un sistema *stateful*—, lo que conlleva una serie de consecuencias relevantes sobre diversos atributos de calidad.

- *Acoplamiento con la instancia del servidor:*  
  Al vincular el estado con una única instancia, las sesiones de usuario y los datos persistentes no pueden compartirse entre instancias.  
  Esto impide la *escalabilidad horizontal*, dado que cada réplica tendría su propio estado local no sincronizado. Implementar una gestión de estado distribuido requeriría una infraestructura adicional (por ejemplo, una base de datos externa o un servicio de caché compartido).

- *Impacto en la disponibilidad y rendimiento percibido:*  
  La existencia de una única instancia con estado convierte al backend en un *punto único de falla*.  
  Si el servidor se detiene, todas las sesiones activas se pierden y no pueden ser recuperadas por otra instancia.  
  Esto afecta la *disponibilidad* y la *experiencia del usuario*, ya que aumenta la percepción de fallas y degradación del rendimiento.

- *Problemas de concurrencia:*  
  Al utilizar el sistema de archivos local como medio de persistencia, se introducen *condiciones de carrera* durante operaciones de lectura y escritura concurrentes.  
  El modelo de concurrencia de Node.js (basado en asincronía) no resulta suficiente para garantizar consistencia, dado que el file system no ofrece bloqueo ni sincronización de accesos concurrentes.  
  Esto impacta negativamente la *confiabilidad* y la *consistencia de datos*.

- *Ausencia de soporte transaccional:*  
  El modelo carece de transacciones, por lo que las operaciones no son atómicas ni recuperables ante fallos.  
  En caso de interrupciones durante la escritura, el sistema puede quedar en estados inconsistentes o requerir restauraciones manuales.  
  Esto degrada tanto la *disponibilidad* como la *recuperabilidad*.

- *Pérdida de integridad de datos:*  
  La falta de atomicidad en las operaciones puede dejar al sistema en estados inválidos (por ejemplo, inconsistencias en saldos o cantidades totales).  
  En consecuencia, la *integridad* del sistema se ve directamente comprometida.

- *Incompatibilidad con balanceo de carga:*  
  Dado que el modelo de persistencia no soporta replicación, la existencia de un balanceador de carga (como Nginx) se vuelve una decisión cuestionable.  
  No existen múltiples backends entre los cuales distribuir tráfico, y el balanceador introduce una capa de comunicación adicional que *degrada el rendimiento* sin aportar beneficios reales.

En conjunto, este modelo de persistencia afecta negativamente la *disponibilidad*, la *escalabilidad*, la *integridad* y la *mantenibilidad*, al tiempo que incrementa la *complejidad operativa* y el riesgo de errores durante la evolución del sistema.

** Instancias únicas de cada servicio

El sistema fue diseñado de manera que cada servicio (API, proxy inverso Nginx, almacenamiento local, etc.) cuenta con una única instancia activa.  
Esta decisión genera *múltiples puntos únicos de falla* y limita severamente la capacidad del sistema para mantener su operación ante fallos parciales.

- Si cualquiera de estos servicios se detiene, el sistema completo se vuelve *indisponible*, afectando directamente la *disponibilidad* y la *tolerancia a fallos*.  
- La ausencia de mecanismos automáticos de recuperación o reinicio (como *health checks*, *watchdogs* o políticas de *restart* configuradas en Docker) agrava el impacto de las fallas, ya que se requiere intervención manual para restablecer el servicio.  
- Tampoco existen estrategias de *replicación*, *balanceo* ni *redundancia*, lo que hace imposible sostener niveles de servicio adecuados bajo carga o ante degradación de componentes.

Esta configuración puede ser suficiente para entornos de desarrollo o demostración, pero resulta *inadecuada para entornos de producción*, donde la *disponibilidad*, *resiliencia* y *recuperabilidad* son atributos esenciales.

** Ausencia de un patrón de arquitectura interna

El sistema carece de un patrón de arquitectura claramente definido a nivel interno (por ejemplo, MVC, capas o microservicios), lo cual genera una estructura *monolítica y fuertemente acoplada*.  
Esta decisión afecta negativamente atributos clave del sistema relacionados con su evolución y mantenibilidad.

- *Dificultad para modificar o extender funcionalidades:*  
  La ausencia de separación de responsabilidades y de interfaces desacopladas complica la incorporación de nuevas funcionalidades (*extensibilidad*) o la modificación segura de las existentes (*modificabilidad*).

- *Incremento en la complejidad del código:*  
  La lógica de negocio, de presentación y de persistencia tienden a mezclarse, lo que eleva la *complejidad cognitiva* y el riesgo de introducir errores.

- *Falta de testabilidad:*  
  Al no existir módulos claramente delimitados, las pruebas unitarias o de integración se vuelven difíciles de implementar, afectando la *testeabilidad* del sistema.

- *Escasa capacidad de evolución:*  
  La arquitectura monolítica limita la posibilidad de migrar gradualmente a tecnologías más modernas o de reestructurar componentes de forma incremental.

En conjunto, esta ausencia de estructura arquitectónica limita la *mantenibilidad*, *evolutividad*, *testeabilidad* y *extensibilidad*, dificultando la gestión del ciclo de vida del software.

** Diagrama C&C inicial.

#+BEGIN_SRC plantuml :file assets/componentes.png :exports results
@startuml
!theme plain
skinparam componentStyle rectangle

component "Cliente" as Client

component "Contenedores Docker" {
 portin "5555" as p_nginx_host

 portin "8090" as p_graphite_host
 portin "8125" as p_graphite_statsd_host
 portin "8126" as p_graphite_statsd_admin

 portin "80" as p_grafana_host

 portin "8080" as p_cadvisor_host

 component "Nginx\n(Reverse Proxy)" as Nginx {
   portin "80" as p_nginx_docker
 }
 component "API\n(Node.js)" as API {
   portin "3000" as p_api_docker
 }
 database "Base de Datos" as DB {
   folder "app/state" {
     [accounts.json]
     [log.json]
     [rates.json]
   }
 }

 component "cAdvisor\n(Monitoring)" as CAdvisor {
   portin "8080" as p_cadvisor_docker
 }
 component "Graphite\n(Métricas)" as Graphite {
   portin "80" as p_graphite_http_docker
   portin "8125" as p_graphite_statsd_docker
   portin "8126" as p_graphite_statds_admin_docker
 }
 component "Grafana\n(Dashboard)" as Grafana {
   portin "3000" as p_grafana_docker
 }
}

Client --> p_nginx_host : HTTP
p_nginx_host --> p_nginx_docker : HTTP
Nginx --> p_api_docker : HTTP
API --> DB : I/O de archivos

p_graphite_host --> p_graphite_http_docker
p_graphite_statsd_host --> p_graphite_statsd_docker
p_graphite_statsd_admin --> p_graphite_statds_admin_docker
p_grafana_host --> p_grafana_docker
p_cadvisor_host --> p_cadvisor_docker
#+END_SRC

#+RESULTS:
[[file:assets/componentes.png]]


** Crítica a arquitectura base.

* Metodología de pruebas

** Recolección de datos

*** StatsD + Graphit + Grafana

*** DataDog

** Generación de carga

* Resultados – Caso base

** Análisis del endpoint Rates

*** Prueba con carga baja

Se realiza una prueba de carga con los siguientes parámetros usando la herramienta Artillery:

#+BEGIN_SRC yaml :eval no
phases:
  - name: Ramp
    duration: 30
    arrivalRate: 1
    rampTo: 5
  - name: Plain
    duration: 60
    arrivalRate: 5
#+END_SRC

**** Resultados observados

[[file:./assets/rates1rps.png]]

Para el escenario con una carga baja se observa el crecimiento sostenido y un posterior estado en el cual se mantiene constante la cantidad de request realizadas al servidor. Se observa que el máximo de la cantidad reportada de request son unas 50 y una media de 36.3 request por segundo.

[[file:./assets/rates1state.png]]

También se logra observar que en un escenario con carga baja, el servidor consigue responder satisfactoriamente a todas las consultas realizadas sin presentar fallos.

[[file:./assets/rates1rpt.png]]

Del lado del cliente se aprecian dos medidas en el tiempo de respuesta, el máximo registrado y la media del tiempo de respuesta. En la totalidad de la prueba de carga se la media se mantiene en un rango de 4-6 ms sin variar de manera brusca, a su vez, el máximo del tiempo de respuesta tiene una media 19.6 ms y se reduce drásticamente en el momento en el que el servidor alcanza su máxima cantidad de carga, mismo punto en el que alcanza un máximo, el cual corresponde a 28.2 ms.

[[file:./assets/rates1resources.png]]

Finalmente en un escenario de carga baja, el sistema se encuentra usando recursos casi constantes, las variaciones en el uso del CPU se encuentra en un rango de 0.5% a 0.6% y la memoria a su vez usa un porcentaje aún menor encontrándose en un rango de 0.071% a 0.074%.

En conclusión, analizando los gráficos y leyendo el resumen dado del comportamiento de la API, se realizaron 390 consultas al endpoint ~rates~ de las cuales todas fueron contestadas de manera satisfactoria.

*** Prueba con mayor carga

A continuación se realiza una prueba de estrés más intensa sobre la API, específicamente en el endpoint ~rates~, con el objetivo de analizar el comportamiento del sistema bajo condiciones de alta demanda y cómo esto impacta en los atributos de calidad, tales como disponibilidad, rendimiento y uso de recursos.

Se incrementa significativamente la cantidad de solicitudes por segundo, simulando un escenario donde múltiples usuarios acceden simultáneamente al servicio. Los resultados permiten identificar el punto de saturación del sistema, posibles errores en las respuestas y variaciones en los tiempos de respuesta y consumo de recursos.

Para esto se recurre nuevamente a la herramienta Artillery, esta vez modificando la configuración anterior por la siguiente:

#+BEGIN_SRC yaml :eval no
phases:
  - name: Ramp
    duration: 30
    arrivalRate: 0
    rampTo: 1000
  - name: Plain
    duration: 60
    arrivalRate: 600
#+END_SRC

**** Resultados observados

[[file:./assets/rates2rps.png]]

En un escenario de mayor carga se observa un crecimiento sostenido en la cantidad de request por segundos, una reducción y estabilización de la misma, en esta ocasión el gráfico no permite realizar conclusiones sobre el estado de la aplicación.

[[file:./assets/rates2state.png]]

En cambio, en el gráfico del estado de las respuestas se observa un crecimiento en la cantidad de respuestas correctas del servidor, pero durante la etapa de llegada constante de la cantidad de request por segundo se observa la aparición de casos de error en las respuestas a los clientes, indicando claramente que el servidor alcanza un límite en la cantidad de clientes que puede atender.

[[file:./assets/rates2rpt.png]]

Para el tiempo de respuestas se observa que una vez el servidor empieza a responder con códigos de error para los clientes, inicia un crecimiento acelerado en los máximos de tiempo de respuesta registrados, para el máximo, la media alcanza 1.46s y el máximo ahora alcanza casi los 10s para poder completar una consulta, mientras que para la media, se observa un crecimiento parecido. Esto indica claramente que el servidor presenta saturación de clientes y no permite responder adecuadamente a todos los clientes que intentan realizar una consulta sobre este endpoint.

[[file:./assets/rates2resources.png]]

A su vez, los resultados observados para los recursos utilizados, en primer lugar para la memoria, se observa un aumento en comparación al test anterior pero no representa un uso excesivo de la misma, haciendo uso de la memoria RAM en un rango de 0.188% a 0.194%. Por el contrario esta vez el CPU alcanza el máximo de su uso rápidamente, llegando a usar un 35.7%, esto antes de alcanzar el máximo en la cantidad de request realizados al servidor, sin embargo a partir de haber alcanzado el límite el uso del CPU desciende rápidamente y a pesar de anteriormente haber observado respuestas con errores en los clientes y no se observaría una relación con el uso de recursos excesivos, es decir, la aplicación no alcanza un límite en el uso de los recursos disponibles.

**** Conclusión

En conclusión en un escenario de carga alta el servidor no es capaz de atender a todos los clientes de manera eficiente y afectando completamente la disponibilidad del servicio, el cual es un atributo de calidad clave y uno de los que se desea mejorar para incrementar la percepción positiva de la aplicación por parte de los clientes.

*** Resumen de métricas de Artillery

Para la prueba de carga baja (salidarates1.txt):

- Total de solicitudes: 390
- Todas respondidas con código 200 (sin fallos)
- Tiempo de respuesta medio: 1.1 ms, mediana: 1 ms, p95: 2 ms, p99: 2 ms

Para la prueba de carga alta (salidarates2.txt):

- Total de solicitudes: 45000
- Respuestas exitosas: 35944 (con 9056 errores ECONNRESET)
- Tiempo de respuesta medio: 321.2 ms, mediana: 16 ms, p95: 2836.2 ms, p99: 4492.8 ms

* Propuestas de mejora

** Implementacion de Valkey como persistencia

*** Tactica aplicada

La implementación utiliza Valkey como almacén de datos centralizado, reemplazando la persistencia en archivos JSON. Los datos se almacenan como claves en Redis:

- ~accounts~: Almacena la lista de cuentas de usuario en formato JSON.

- ~rates~: Contiene las tasas de cambio entre monedas.

- ~log~: Registra el historial de transacciones realizadas.

El módulo ~valkey.js~ proporciona funciones asíncronas para inicializar la conexión (~init()~), obtener datos (~getAccounts()~, ~getRates()~, ~getLog()~) y actualizarlos (~setAccounts()~, ~setRates()~, ~setLog()~). Estas funciones serializan/deserializan los datos a JSON para almacenarlos como strings en Redis.

En ~exchange.js~, se importa y utiliza este módulo para todas las operaciones de persistencia, reemplazando las lecturas/escrituras directas a archivos. La inicialización se realiza al inicio de la aplicación con ~await valkeyInit()~.

*** Configuracion

Se agregó un servicio ~valkey~ en el ~docker-compose.yml~ utilizando la imagen ~valkey/valkey:8.1.4-alpine~, expuesto en el puerto 6379. La aplicación se conecta mediante la variable de entorno ~VALKEY_URL=redis://valkey:6379~.

*** Beneficios

Al centralizar el estado en Valkey, múltiples instancias de la API pueden compartir el mismo almacén de datos. Esto elimina la dependencia de estado local en memoria o archivos, permitiendo:

- Escalado horizontal sin pérdida de consistencia.

- Persistencia real de los datos, sobreviviente a reinicios de contenedores.

- Operaciones atómicas en Redis para transacciones financieras.

Esta táctica mejora significativamente la Disponibilidad y Escalabilidad, mitigando los puntos únicos de falla relacionados con la persistencia local.

#+BEGIN_SRC plantuml :file assets/componentes-modificados.png :exports results
@startuml
!theme plain
skinparam componentStyle rectangle

component "Cliente" as Client

component "Contenedores Docker" {
  portin "5555" as p_nginx_host

  portin "8090" as p_graphite_host
  portin "8125" as p_graphite_statsd_host
  portin "8126" as p_graphite_statsd_admin

  portin "80" as p_grafana_host

  portin "8080" as p_cadvisor_host

  component "Nginx\n(Reverse Proxy)" as Nginx {
    portin "80" as p_nginx_docker
  }
  component "API Node 1\n(Node.js)" as API1 {
    portin "3000" as p_api1_docker
  }
  component "API Node 2\n(Node.js)" as API2 {
    portin "3001" as p_api2_docker
  }
  component "API Node 3\n(Node.js)" as API3 {
    portin "3002" as p_api3_docker
  }
  database "Valkey\n(Redis)" as Valkey {
    portin "6379" as p_valkey_docker
  }

  component "cAdvisor\n(Monitoring)" as CAdvisor {
    portin "8080" as p_cadvisor_docker
  }
  component "Graphite\n(Métricas)" as Graphite {
    portin "80" as p_graphite_http_docker
    portin "8125" as p_graphite_statsd_docker
    portin "8126" as p_graphite_statds_admin_docker
  }
  component "Grafana\n(Dashboard)" as Grafana {
    portin "3000" as p_grafana_docker
  }
}

Client --> p_nginx_host : HTTP
p_nginx_host --> p_nginx_docker : HTTP
Nginx --> p_api1_docker : HTTP
Nginx --> p_api2_docker : HTTP
Nginx --> p_api3_docker : HTTP

API1 --> p_valkey_docker : Redis
API2 --> p_valkey_docker : Redis
API3 --> p_valkey_docker : Redis

p_graphite_host --> p_graphite_http_docker
p_graphite_statsd_host --> p_graphite_statsd_docker
p_graphite_statsd_admin --> p_graphite_statds_admin_docker
p_grafana_host --> p_grafana_docker
p_cadvisor_host --> p_cadvisor_docker
#+END_SRC

#+RESULTS:
[[file:assets/componentes-modificados.png]]


** Implementacion de PostgreSQL como persistencia

*** Tactica aplicada

La implementación utiliza PostgreSQL como almacén de datos relacional, reemplazando la persistencia en archivos JSON y Valkey. Las tablas creadas son:

- ~accounts~: Almacena las cuentas de usuario con campos como id, currency, balance, created_at, updated_at, deleted.

- ~exchange_rates~: Contiene las tasas de cambio entre monedas con base_currency, counter_currency, rate, updated_at.

- ~transactions~: Registra el historial de transacciones realizadas, con soporte para atomicidad en operaciones de intercambio.

El módulo ~databaseAdapter.js~ proporciona funciones para conectarse a PostgreSQL usando el paquete ~pg~, manejando conexiones y transacciones. Los modelos en ~models/~ (Account, ExchangeRate, Transaction) manejan las operaciones CRUD con soporte para transacciones ACID.

En ~exchange.js~, se utilizan estos modelos para todas las operaciones financieras, incluyendo transacciones atómicas para intercambios que requieren consistencia (ej. actualizar balances y registrar transacción en una sola operación).

*** Configuracion

Se agregó un servicio ~postgres~ en el ~docker-compose.yml~ utilizando la imagen ~postgres:15-alpine~, con inicialización de la base de datos mediante el script ~01-init.sql~ que crea las tablas, índices y datos iniciales.

Se configuraron tres instancias de la API (~api1~, ~api2~, ~api3~) conectadas a PostgreSQL, permitiendo escalado horizontal sin pérdida de estado.

Se actualizó ~nginx_reverse_proxy.conf~ para balancear carga entre las tres instancias de API utilizando un bloque ~upstream~.

*** Beneficios

Al centralizar el estado en PostgreSQL con transacciones ACID, múltiples instancias pueden compartir el mismo almacén de datos de forma consistente y atómica. Esto elimina dependencias de estado local, permite escalado horizontal sin pérdida de consistencia, y asegura atomicidad en operaciones financieras críticas, mejorando la integridad de datos.

Esta táctica mejora significativamente la Disponibilidad (reduciendo puntos únicos de falla en persistencia), Escalabilidad (permitiendo más nodos con estado compartido), y Performance (con transacciones eficientes, concurrencia controlada y consultas optimizadas con índices).

#+BEGIN_SRC plantuml :file assets/componentes-postgresql.png :exports results
@startuml
!theme plain
skinparam componentStyle rectangle

component "Cliente" as Client

component "Contenedores Docker" {
  portin "5555" as p_nginx_host

  portin "8090" as p_graphite_host
  portin "8125" as p_graphite_statsd_host
  portin "8126" as p_graphite_statsd_admin

  portin "80" as p_grafana_host

  portin "8080" as p_cadvisor_host

  component "Nginx\n(Reverse Proxy)" as Nginx {
    portin "80" as p_nginx_docker
  }
  component "API Node 1\n(Node.js)" as API1 {
    portin "3000" as p_api1_docker
  }
  component "API Node 2\n(Node.js)" as API2 {
    portin "3001" as p_api2_docker
  }
  component "API Node 3\n(Node.js)" as API3 {
    portin "3002" as p_api3_docker
  }
  database "PostgreSQL" as PostgreSQL {
    folder "exchange_db" {
      [accounts]
      [exchange_rates]
      [transactions]
    }
    portin "5432" as p_postgres_docker
  }

  component "cAdvisor\n(Monitoring)" as CAdvisor {
    portin "8080" as p_cadvisor_docker
  }
  component "Graphite\n(Métricas)" as Graphite {
    portin "80" as p_graphite_http_docker
    portin "8125" as p_graphite_statsd_docker
    portin "8126" as p_graphite_statds_admin_docker
  }
  component "Grafana\n(Dashboard)" as Grafana {
    portin "3000" as p_grafana_docker
  }
}

Client --> p_nginx_host : HTTP
p_nginx_host --> p_nginx_docker : HTTP
Nginx --> p_api1_docker : HTTP
Nginx --> p_api2_docker : HTTP
Nginx --> p_api3_docker : HTTP

API1 --> p_postgres_docker : PostgreSQL
API2 --> p_postgres_docker : PostgreSQL
API3 --> p_postgres_docker : PostgreSQL

p_graphite_host --> p_graphite_http_docker
p_graphite_statsd_host --> p_graphite_statsd_docker
p_graphite_statsd_admin --> p_graphite_statds_admin_docker
p_grafana_host --> p_grafana_docker
p_cadvisor_host --> p_cadvisor_docker
#+END_SRC

#+RESULTS:
[[file:assets/componentes-postgresql.png]]


** Comparacion de estados de requests

#+CAPTION: Estado de requests en la arquitectura base
[[file:assets/request-state-base.png]]

En la arquitectura base, se observa una alta tasa de errores debido a la sobrecarga de la única instancia de la API y problemas de concurrencia en el acceso a archivos JSON locales, lo que resulta en fallos de conexión y respuestas erróneas bajo carga elevada.

#+CAPTION: Estado de requests con la propuesta de mejora (PostgreSQL y balanceo de carga)
[[file:assets/request-state-postgresql.png]]

Con la propuesta de mejora que incluye balanceo de carga entre tres nodos y PostgreSQL como persistencia, la tasa de errores se reduce significativamente. El balanceo de carga distribuye la carga uniformemente entre los nodos, evitando la saturación de un solo punto, mientras que PostgreSQL maneja mejor la concurrencia mediante transacciones ACID y acceso controlado a la base de datos, minimizando errores por conflictos de acceso a datos y mejorando la estabilidad general del sistema.

* Trade-offs detectados.

* Pedido Adicional (Volumen de transacciones por moneda)

* Conclusiones
